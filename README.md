# 宝可梦集换式卡牌官网爬虫

这是一个基于 Node.js 和 Puppeteer 构建的强大网络爬虫，专门用于抓取宝可梦集换式卡牌游戏亚洲官网（`asia.pokemon-card.com`）的卡牌数据。

该项目从一个简单的链接提取脚本开始，经过多次迭代，现已发展成为一个功能完备、稳定且高效的数据采集工具。它能够系统地抓取网站上的所有卡牌信息，并将其整理成结构化的 JSON 文件，同时下载相关的图片资源。

## ✨ 主要功能

  - **全卡种支持**:能够自动识别并分别处理**宝可梦卡**、**训练家卡**和**能量卡**，并为不同类型的卡牌使用最合适的 JSON 结构。
  - **数据完整性**: 抓取包括基本信息、HP、技能、属性、进化链、图鉴信息、卡包信息在内的详尽字段。
  - **图片下载**: 自动下载每张卡牌的高清大图和其所属的卡包符号图标，并分别存放在 `card-images` 和 `expansion-symbol-images` 目录中。
  - **智能翻页**: 自动检测网站的总页数，并支持从任意指定页面开始，完成所有分页的遍历抓取，确保数据无遗漏。
  - **并发处理**: 支持配置并发数量，可同时处理多个详情页的抓取任务，极大地提升了数据采集效率。
  - **高容错性**: 采用“边翻页边处理”的模式，并将数据实时增量写入 `.jsonl` 文件。即使在长时间运行中意外中断，也只会丢失少量正在处理的数据，保证了已抓取数据的安全。
  - **双格式输出**: 最终数据会同时提供两种格式：
    1.  `pokemon_cards.jsonl` (JSON Lines): 方便流式处理和灾难恢复。
    2.  `pokemon_cards.json`: 格式化后的标准 JSON 数组，方便直接在其他应用中导入和使用。
  - **易于配置**: 所有核心参数（如并发数、文件名、起始URL）都集中在文件顶部的 `CONFIG` 对象中，方便用户按需调整。

## 🛠️ 技术栈

  - **[Node.js](https://nodejs.org/)**: 脚本的运行环境。
  - **[Puppeteer](https://pptr.dev/)**: 核心库，用于驱动一个无头（Headless）Chrome 浏览器来进行网页的导航、交互和数据提取。

## 📁 文件结构

成功运行脚本后，您的项目目录将如下所示：

```
.
├── card-images/                # 存放已下载的卡牌图片
│   ├── hk00012345.png
│   └── ...
├── expansion-symbol-images/    # 存放已下载的卡包符号图片
│   ├── svaw_f.png
│   └── ...
├── node_modules/
├── pokemon_cards.json          # 最终输出的标准JSON文件
├── pokemon_cards.jsonl         # 实时写入的JSONL文件
├── package.json
├── package-lock.json
└── scraper.js                  # 爬虫主脚本
```

## 🚀 安装与运行

### 1\. 环境准备

请确保您的电脑上已经安装了 [Node.js](https://nodejs.org/) (推荐 v20 或更高版本)。

### 2\. 安装依赖

在您的项目根目录下，打开终端并运行以下命令来安装 Puppeteer：

```bash
npm install puppeteer
```

### 3\. 配置（可选）

打开 `scraper.js` 文件，您可以在文件顶部的 `CONFIG` 对象中修改各项配置：

```javascript
const CONFIG = {
  // 并发处理详情页的数量
  CONCURRENT_PAGES: 5, 
  // 卡牌图片存储的目录名
  CARD_IMAGE_DIR: 'card-images',
  // 卡包符号图片存储的目录名
  EXPANSION_SYMBOL_IMAGE_DIR: 'expansion-symbol-images',
  // ...其他配置
  START_URL: 'https://asia.pokemon-card.com/hk/card-search/list/',
};
```

### 4\. 运行爬虫

一切准备就绪后，运行以下命令启动爬虫：

```bash
node scraper.js
```

爬虫会开始执行，并在控制台输出详细的抓取日志。全部任务完成后，您就可以在项目中找到抓取到的数据和图片了。

## 📝 注意事项

  - 本项目仅供学习和技术研究使用，请勿用于商业目的。
  - 爬虫运行时会消耗较多的网络和计算资源。
  - 请尊重目标网站的版权和 robots.txt 协议，不要设置过高的并发数，以免对目标服务器造成过大压力。
  - 如果目标网站的页面结构发生变化，可能会导致此爬虫失效，届时需要更新代码中的选择器。